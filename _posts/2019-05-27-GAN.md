---
layout:     post
title:      GAN基础
subtitle:   理论推导
date:       2019-05-27
author:     LT
header-img: 
catalog: true
tags:
    - GAN
    - logistic_regression
---



>与GAN有关的PPT课件、视频、作业要求，参见[李宏毅-课程主页](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS18.html)


### 前置知识
极大似然估计、二分类、auto-encoder

### Introduction
+ G generator：只学习细节，不知道全局
如果单独使用G来生成图像，VAE（variational auto-encoder）中的decoder就是我们想要的G。
VAE的过程是：**input image**---encoder--->**code**---decoder--->**output image**
但是这里的G有个问题：只学习细节，不知道全局（比如像素之间的相关性）。that's to say,采用模型中的解码端作为G，本质是在生成训练集已有的数据类似的数据，无法生成训练集中没有的数据。

+ D discriminator：
只学习全局，不知道细节。
只有positive example，没有negative example。
如果单独使用D来生成图像（穷举所有的pixel？），那么不管生成什么样的图像，都会被认为是positive example。

+ 将G和D结合起来->GAN
实验证明，GAN生成的图像质量比VAE要好。


### Basic idea
Pdata是真实数据的分布，PG是生成数据的分布。G是生成网络Generator，D是判别网络Discriminator。开山作[paper](https://arxiv.org/abs/1406.2661)中的value function V(D, G)定义如下。（可以理解为PG和Pdata之间的divergence）

$$ V(D, G)=E_{ x\sim p_{data}(x)}[\log D(x)]+ {E}_{z\sim p_{z}(z)}[\log (1-D(G(z)))]$$

$$D^{*}=  \max _{D} V(D, G)$$

$$\begin{align}
G^{*} &= \min _{G} D^{*} \\
&= \min _{G} {E}_{z \sim p_{z}({z})}[\log (1-D(G({z})))] \\
&= \max _{G} {E}_{z \sim p_{z}({z})}[\log D(G({z}))]
\end{align}$$

步骤：
1. 初始化G和D的相关参数
2. 先fix G 训练Discriminator，使得divergence越大越好(便于分类)；
3. 再fix D(此时V(D, G)中的第一项是常数，计算时无需考虑了)训练Generator，使得divergence越小越好。

### Discriminator与binary classifier
D其实就是一个二分类器(可以是使用了sigmoid作为输出的深层网络)，来自Pdata的x是正例，label=1；来自PG的x'=G(z)是反例，label=0。


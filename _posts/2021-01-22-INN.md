---
layout:     post
title:      可逆网络
subtitle:   flow👉condition flow
date:       2021-01-22
author:     LT
header-img: 
catalog: true
tags:
    - 深度学习
    - flow
---

## 可逆网络（flow）动机+特点
### 观点一，和GAN/VAE/自回归相比，更好地建模未知分布
1. 我们知道，由[贝叶斯派](./2020-05-01-频率-贝叶斯.md)发展而来的是概率图模型。
    - 概率图分为三个理论部分：表示、推断、学习（参数学习、结构学习）。
    - 在参数学习里，带有隐变量的模型，称之为latent variable model。

2. latent variable model的核心问题是，由于p(x)维度太高，积分难求。（这也是贝叶斯派要解决的问题）
    - 根据贝叶斯定理，posterior = prior*likelihood
    - $ p(x)= \int p(x,z) dz \\
            = \int p(z)p(x|z) dz $

3. 为了解决p(x)难求的问题，各种生成模型都有各自的解决方法。
    - GAN的做法[请移步](./2020-04-10-MLE和GAN和flow.md)；
    - GMM做法是通过EM来求p(x)最大值；
    - VAE做法是变分推断，最大化ELBO。

4. 为了使p(x)能够计算，flow做了两件事：
    - 利用**change of variables**，构造具有解析式的p(x)
        * $ p(x) = \pi (z) | det \frac{\partial z}{\partial x} | $
        * 其中，$ z= f_\theta(x)$，MLE可以求得这个θ。
    - p(x)构造出来了，那么怎么采样$x_i$？👉**可逆**思想
        * 直观的想法是，由pdf(概率密度函数,这里就是p(x))计算CDF(累积分布函数)，从0-1之间生成随机数，由`CDF的逆`，求出随机数对应的$x_i$。
        * 但是，这种方法只适用于服从简单分布的pdf。这里，由于p(x)解析式里含有$f_\theta$，$f_\theta$是复杂网络，所以，`CDF`和`CDF的逆`都很难求。
        * flow的做法是，设计耦合层，使得$f_\theta$是**可逆**的。即$ x= f_\theta^{-1}(z)$。

### 观点二，和前馈网络相比，将“测量→参数”问题，模糊→确定
- 对于`从观察测量值中确定隐参数`任务，
    - 通常，从参数到测量空间的正向过程是明确定义的，而反问题是不明确的：多个参数集可以导致相同的测量。
    - 为了充分描述这种模糊性，必须确定以观察测量为条件的完整后验参数分布：INN很适合这个任务。
- INN
    - INN专注于学习前向过程，使用额外的潜在输出变量Z来捕获前向丢失的信息。
    - 由于可逆性，隐式地学习了相应的反过程模型。
    - 给定观察测量和Z分布，INN的逆传递提供了完整的后验参数空间。


## 笔记下载：flow，condition flow
2. 三个flow经典模型，我整理到PPT里，[点击下载](https://documents-1300025586.cos.ap-nanjing.myqcloud.com/01-NICE_realNVP_Glow_2020-05-03.pptx)

3. 更多：条件flow的应用，[点击下载](https://documents-1300025586.cos.ap-nanjing.myqcloud.com/02-%E6%9D%A1%E4%BB%B6flow_2020-06-29.pptx)

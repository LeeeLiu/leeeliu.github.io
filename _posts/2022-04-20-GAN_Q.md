---
layout:     post
title:      GAN带来的问题和解决方法
subtitle:   理论推导
date:       2022-04-20
author:     LT
header-img: 
catalog: true
tags:
    - GAN
    - 深度学习
    - 理论篇
---

### GAN带来的问题、原因、解决方法
0. 两类损失
    - loss1 = log(1-D(G(z)))
        - 训练初期下降慢
        - G梯度消失：在D*下，等价于JS。

    - loss2 = -log(D(G(z)))
        - G梯度不稳：在D*下，等价于 KL - 2JS。
        - 缺乏多样性：KL不对称。
1. 问题一：训练不稳定
    - 原因
        1. 训练初期下降慢： loss1 =log(1-D(G(z))) 的函数曲线，一开始是下降的慢。
        2. G梯度消失：最优判别器 D* 代入 loss1 就是JS距离。JS问题是，当真实数据x和生成数据G (z)不重叠，不论x和G(z)距离远近，JS=log2，梯度是0。在训练初期，x和G(z)肯定是不重叠的，这样D很容易区分，G就得不到有效的学习梯度了。 
        3. G梯度不稳：对于 loss2 = -log(D(G(z)))。这个在一开始可以下降的快。但是，此时，这种 loss2 本质上 = KL - 2JS。这将为训练 G 带来不稳定的梯度。

    - 方法
        1. 损失角度
            - LS-GAN 最小二乘 把0或1当GT
                - 去掉sigmoid
                    - 因为，当D训的太好（或者说当x和G(z)不重叠），对于G(z)打分恒为0，对于x打分恒为1，这样就无法给G提供梯度。所以，干脆使用线性层。
            - W-GAN，EM距离，找到一个比JS更好的度量
                - 去掉sigmoid，去掉log
                - 控制判别器的参数绝对值小于c
                    - 因为EM距离没有上下界
                - 别用基于动量的优化算法（momentum和Adam），推荐RMSProp，SGD
        2. 更新频率：D多更新几次之后，G再更新
        3. 数据角度：检查训练数据，分布是否平衡
2. 问题二：模式坍塌（mode collapse）
    - 概念：生成数据种类单一，缺乏多样性。
    - 原因： loss2 里，KL距离不对称，生成器宁可丧失多样性也不愿丧失准确性
        - 当Pdata=0/1，Pg=1/0，对G的两种误差的惩罚完全不同。
        第一种误差是 G `产生了不真实的样本`，对应的惩罚很大。
        第二种误差是 G `不能产生真实样本`，而惩罚很小。
        第一种误差是缺乏准确性，而第二种误差是缺乏多样性。
        基于此，G 倾向于生成重复但是安全的样本，而不愿意冒险生成不同但不安全的样本。
    - 方法：
        - 不用KL距离，W-GAN
    - 参考 
    [GAN综述](https://mp.weixin.qq.com/s/iLAE_WR-rQrqd4dtYWB_gA)
    [令人拍案叫绝的Wasserstein GAN](https://zhuanlan.zhihu.com/p/25071913)

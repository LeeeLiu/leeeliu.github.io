---
layout:     post
title:      代价函数、二分类问题
subtitle:   交叉熵、相对熵（KL散度）、对率回归
date:       2019-05-10
author:     LT
header-img: 
catalog: true
tags:
    - 机器学习
    - 理论篇
---



关于熵的总结，来自吴军博士的《数学之美》。
### 熵
1. 熵越大，随机变量的不确定性越大，信息量就越大。
2. 熵增原理：宇宙所有事物自发地朝着无序方向发展，即，熵会不断增加。
3. `无序`比有序的事物`熵更大`。
4. 最大熵：
    - 目的：保留全部不确定性，把风险降低到最小。即概率分布最均匀。
    - 当我们遇到不确定性时，就要保留各种可能性。即，在满足已知条件下，不要对未知情况做任何主观假设。
    - 朴素例子：不要把鸡蛋放在一个篮子里。
### 互信息/信息增益
- 特征A对训练数据集D的信息增益g(D,A)=H(D)-H(D|A)。即熵H(D)，与条件熵H(D|A)之差。表示得知特征A的信息而使得训练集D的信息的不确定性减少的程度。
- 直觉上来说，小概率事件比大概率事件包含的信息量大。“百年一见”比“习以为常”事件包含的信息量大。

### 交叉熵H(p,q)
1. 为什么会有交叉熵代价函数（Binary Cross-Entropy）：它可以解决均方误差（MSE，mean squared error）函数权重更新过慢的问题。
2. 非对称

$$H(p, q)=\sum_{i} p(i)  \log \frac{1}{q(i)}$$

### 相对熵KL(p||q)
1. **相对熵**，又称**KL散度**(Kullback–Leibler divergence,KLD)。
**相对熵**在一些文献中也称为**交叉熵**。这是因为在一些优化问题场景中（比如DL里最小化loss）：
>真实分布`H(p)`固定，`KL(p||q)`仅仅由`H(p,q)`决定（公式如下）。`H(p,q)`的表达更简单，因此选择**优化交叉熵H(p,q)**。q可以是sigmoid/softmax输出值。以二分类为例。（y是真实标签(+1或-1)，`p(y|x)`是sigmoid输出值，`L( w, b )`是loss，详见“对率回归”部分）
`H( y, p(y|x) ) = L( w, b )`可以理解为：在所有正例(y=1)和负例(y= -1)中，**真实标签y**和**数据x推出真实标签y的概率**的差别。

2. KL散度有非对称性，即`KL(p||q)≠KL(q||p)`。
3. KL表示两个函数或概率分布的差异性：差异越大则相对熵越大，差异越小则相对熵越小。特别地，若两者相同则熵为0。
4. **相对熵 = 交叉熵 - 信息熵**。

$$\begin{align} KL(p,q) &=H(p,q)-H(p) \\ &=\sum_{k=1}^{N} p_{k} \log _{2} \frac{1}{q_{k}}-\sum_{k=1}^{N} p_{k} \log _{2} \frac{1}{p_{k}} \\ &=\sum_{k=1}^{N} p_{k} \log _{2} \frac{p_{k}}{q_{k}} \end{align}$$

### 对率回归-logistic regression
sigmoid函数：
$$\begin{align}
\theta(z) &=\frac{1}{1+e^{-z}} \\
f(x) &=\theta(z(x)) = \frac{1}{1+e^{-\left(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}+b\right)}}
\end{align}$$

假设对率回归使用的输入是x和label y，输出是f(x)。f(x)拟合的是给定x，输出对应label y的概率`p(y|x)`。这个概率≥0.5时，判断y=1；否则y=0。

$$p(y|x)=
\begin{cases}
f(x)    & {  for \ y=+1} \\ 
1-f(x)  & {  for \ y=-1}
\end{cases}$$

函数f有个很好的性质：1-f(x)=f(-x)。所以:

$$\begin{align}
p(y_{i}|x_{i}) &= y_{i}f(+x_{i}) + (1-y_{i})(1-f(+x_{i})) \\
 &= f(y_{i} × x_{i}) 
\end{align}$$

构造loss function的过程如下：
1. 极大似然估计（利用已知的样本结果，反推最有可能（最大概率）导致这样结果的模型的参数值θ，比如w和b），maximize L(w,b)：

$$L(w, b)= \prod_{i=1}^{m} p\left(y_{i} | {x}_{i} ; {w}, b\right)$$
2. 引入**log操作**，让连乘转化为连加：

$$L(w, b)=\sum_{i=1}^{m} \log p\left(y_{i} | {x}_{i} ; {w}, b\right)$$
3. 为了将maximize问题转化为minimize问题，添加一个**负号**，并引入平均数操作1/m。通过gradient descent求解。

$$L(w, b)=-\frac{1}{m} \sum_{i=1}^{m}\left[y_{i} \log f(x_{i})+\left(1-y_{i}\right) \log (1-f(x_{i}))\right]$$

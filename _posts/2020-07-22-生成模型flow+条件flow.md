---
layout:     post
title:      flow👉condition flow
subtitle:   动机、三个经典模型、condition flow的应用领域
date:       2020-07-22
author:     LT
header-img: 
catalog: true
tags:
    - 深度学习
    - flow
---

### Why flow？
1. 我们知道，由[贝叶斯派](./2020-05-01-频率-贝叶斯.md)发展而来的是概率图模型。
    - 概率图分为三个理论部分：表示、推断、学习（参数学习、结构学习）。
    - 在参数学习里，带有隐变量的模型，称之为latent variable model。

2. latent variable model的核心问题是，由于p(x)维度太高，积分难求。（这也是贝叶斯派要解决的问题）
    - 根据贝叶斯定理，posterior = prior*likelihood
    - $ p(x)= \int p(x,z) dz \\
            = \int p(z)p(x|z) dz $

3. 为了解决p(x)难求的问题，各种生成模型都有各自的解决方法。
    - GAN的做法[请移步](./2020-04-10-MLE和GAN和flow.md)；
    - GMM做法是通过EM来求p(x)最大值；
    - VAE做法是变分推断，最大化ELBO。    

### 从flow到condition flow
1. 为了使p(x)能够计算，flow做了两件事：
    - 利用**change of variables**，构造具有解析式的p(x)
        * $ p(x)= \pi(z) |det \frac{\partial z}{\partial x}| $
        * 其中，$ z= f_\theta(x)$，MLE可以求得这个θ。
    - p(x)构造出来了，那么怎么采样$x_i$？👉**可逆**思想
        * 直观的想法是，由pdf(概率密度函数,这里就是p(x))计算CDF(累积分布函数)，从0-1之间生成随机数，由`CDF的逆`，求出随机数对应的$x_i$。
        * 但是，这种方法只适用于服从简单分布的pdf。这里，由于p(x)解析式里含有$f_\theta$，$f_\theta$是复杂网络，所以，`CDF`和`CDF的逆`都很难求。
        * flow的做法是，设计耦合层，使得$f_\theta$是**可逆**的。即$ x= f_\theta^{-1}(z)$。
2. 三个flow经典模型，我整理到PPT里，[点击下载](https://documents-1300025586.cos.ap-nanjing.myqcloud.com/01-NICE_realNVP_Glow_2020-05-03.pptx)

3. 更多：条件flow的应用，[点击下载](https://documents-1300025586.cos.ap-nanjing.myqcloud.com/02-%E6%9D%A1%E4%BB%B6flow_2020-06-29.pptx)

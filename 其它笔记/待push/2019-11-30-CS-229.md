---
layout:     post
title:      CS-229课程记录
subtitle:   机器学习
date:       2019-11-30
author:     LT
header-img: 
catalog: true
tags:
    - 机器学习
    - 理论篇
---


>视频、讲义地址：[吴恩达-机器学习-课程](https://study.163.com/course/courseMain.htm?courseId=1004570029&_trace_c_p_k2_=96a91a1aad2d4ed89212e7a66fb0b3f3)


### 监督/非监督 学习
监督式：
1. 输出是连续：回归问题
2. 输出是离散(0/1)：分类问题
非监督式：新闻话题检测


### 线性回归
Lecture 2、4
1. 线性回归中的代价函数，没有局部最优，只有全局最优。
2. “Batch”: 每一步梯度下降使用所有的训练样本。
3. 多变量线性回归的一些tricks：特征缩放
4. 学习率α太小/大都会让结果很难收敛。

- 解决线性回归问题中的最小化代价函数问题：
1. 梯度下降
2. 正规方程组(最小二乘法)：$\theta=\left(X^{T} X\right)^{-1} X^{T} y$

优缺点/方法|梯度下降|正规方程组
--|:--:|--:
优点|数据规模大也能work well|不需要学习率，不需要迭代
缺点|需要学习率，需要迭代|数据规模大不能work well

### 对率回归
1. 这时候只能梯度下降来解。
2. 多分类：对每个类别i都训练一个分类器$h_{i}(x)$。对于输入x，使得$h_{i}(x)$最大的i就认为是x的类别。
3. advanced optimization没看懂

### 正则化-解决过拟合
Lecture 7
1. 减少特征数量
2. 正则化（降低参数θ的值/幅度）
目标是：最小化$J(\theta)=\frac{1}{2 m}\left[\sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^{2}+\lambda \sum_{j=1}^{n} \theta_{j}^{2}\right]$

### 神经网络表示
lecture 9
1. 二分类：1个输出单元
多分类：k个输出单元（对于k个类别）
2. 反向传播：为了方便计算梯度下降中的导数项(derivatives)。在忽略正则化项的情况下，我们认为$\frac{\partial }{{\partial \theta _{ij}^{(l)}}}J(\theta ) = a_j^{(l)}\delta _{\rm{i}}^{(l + 1)}$（详情见lecture 9）
3. 梯度检验：验证自己写的backprop代码没有bug。检查无误之后一定要关掉梯度检验(它太慢了)。
4. 如果权重全部初始化为0，结果就是每一次迭代以后得到的权重都是相等的（尽管它们不为0了），所以需要随机初始化。
5. 总结
输入单元个数：数据特征的维度
输出单元个数：类别的个数
- 训练步骤：
1. 初始化
2. 前向传播计算$h_{\theta}(x)$
3. 计算代价函数J(θ)
4. 后向传播计算偏导数项
5. 梯度检验，验证后向传播代码是否正确。（检验完毕关闭梯度检验！）
6. 使用后向传播、梯度下降等方法，达到min J(θ)的目标


### 应用机器学习的建议
lecture 10
>为什么非要是**验证集**而不是测试集？
1. 测试集是为了测试模型的泛化性能。如果用测试集来进行超参的选择，那么这个模型将会很拟合这个测试集，导致测试集的存在失去了它自身的意义—泛化。因此，我们设置了验证集来完成这个工作。
2. 人工调参（超参）是根据验证集上的效果来进行的。

>解决high variance-过拟合：
1. 减少特征维度/数量d
2. 增加训练example数量
3. 增大正则化参数-lamda
>解决high bias-欠拟合：
1. 增加特征维度/数量d
2. 增加polynomial特征
3. 减小正则化参数-lamda

### 支持向量机
lecture 12
从logistic regression说起。
1. 两个算法的loss
- logistic regression的最小化目标loss：（h(x)是sigmoid函数）
$\min _{\theta} \frac{1}{m}\left[\sum_{i=1}^{m} y^{(i)}\left(-\log h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right)\left(\left(-\log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}\right.$
- SVM的最小化loss：(cost1、cost0是分段函数，是上面2个log项的近似，计算更加高效)
$\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)} \operatorname{cost}_{1}\left(\theta^{T} x^{(i)}\right)+\left(1-y^{(i)}\right) cost_{0}\left(\theta^{T} x^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}$
2. 两个算法的决策边界：
- logistic regression：如果h(x;θ)≥0.5，即θx≥0，则y=1；如果h(x;θ)<0.5，即θx<0，则y=0。（做决策时候不用h(x)，计算loss时候用）
- SVM：如果θx≥1，则y=1；如果θx≤-1，则y=0。（做决策、计算loss都不用h(x)了）
3. 核函数similarity：解决线性不可分的问题
- m个样本$\left(x^{(1)}, y^{(1)}\right),\left(x^{(2)}, y^{(2)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right)$，每个样本有m个特征$f^{(i)}=\left[f_{1}^{(i)}, f_{2}^{(i)}, \ldots, f_{m}^{(i)}\right]^{T}$，选择m个landmark $l^{(1)}=x^{(1)}, l^{(2)}=x^{(2)}, \ldots, l^{(m)}=x^{(m)}$。
对于某个样本$x^{(i)}$，它的特征$f^{(i)}$中每一项含义是：
$f_{1}^{(i)}= similarity \left(x^{(i)}, l^{(1)}\right)$
$f_{2}^{(i)}= similarity \left(x^{(i)}, l^{(2)}\right)$
$\cdots \cdots$
$f_{m}^{(i)}=similarity\left(x^{(i)}, l^{(m)}\right)$
- 核-SVM-最小化目标loss
$\min _{\theta} C \sum_{i=1}^{m}\left[y^{(i)} \operatorname{cost}_{1}\left(\theta^{T} f^{(i)}\right)+\left(1-y^{(i)}\right) cost_{0}\left(\theta^{T} f^{(i)}\right)\right]+\frac{1}{2} \sum_{i=1}^{n} \theta_{j}^{2}$

- 决策边界
如果 $\theta^{T} f \geq 0$，则y=1，否则y=0
- 根据特征维度n、样本数量m的不同，logistic regression和SVM有各自不同的应用场景。


### K-means algorithm
lecture 13
- 伪代码
randomly initialize K cluster centroids μ1,μ2...μK ∈ R
repeat{
    for i=1 to m
    $c^{(i)}$ = index(from 1 to K)of cluster centroid closest to $x^{(i)}$
    for k=1 to K
    $μ_{k}$ = average(mean)of points assigned to cluster k
}
- k的选择：根据后续具体目标来定。比如T恤尺码划分：S、M、L和XS、S、M、L、XL。


### 降维
lecture 14
- 动机：数据压缩(减少硬盘存储、加快学习速度)、可视化
- 主成分分析(PCA)算法：n维->k维
1. 伪代码
```
Σ=(1/m)*X'*X;
[U,S,V]=SVD(Σ);
Ureduce=U(:,1:k);
z = Ureduce'*x;
```
2. 重构x
`x_approx = Ureduce*z;`
3. 选择最小的k，使得x和x_approx之间的差别最小。
4. 注意：PCA不是prevent overfitting的方式。


### 异常检测
lecture 15
1. 算法-密度估计
>对于全体样本，第i个样本$x^{(i)}$的第j个维度特征的概率分布是$p(x_{j}^{(i)} ; \mu_{j},\sigma_{j}^{2})$，i ~ [1,m]，j ~ [1,n]。对于参数$\mu_{1}, \dots, \mu_{n}, \sigma_{1}^{2}, \ldots, \sigma_{n}^{2}$，计算如下：
$\begin{aligned} \mu_{j} &=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)} \\ \sigma_{j}^{2} &=\frac{1}{m} \sum_{i=1}^{m}\left(x_{j}^{(i)}-\mu_{j}\right)^{2} \end{aligned}$
第i个样本$x^{(i)}$的所有维度特征概率分布$p(x^{(i)})$由$p(x_{j}^{(i)} ; \mu_{j},\sigma_{j}^{2})$累乘得到。
 
>对于一个新的样本x，p(x)计算方法如下。
$p(x)=\prod_{j=1}^{n} p\left(x_{j} ; \mu_{j}, \sigma_{j}^{2}\right)=\prod_{j=1}^{n} \frac{1}{\sqrt{2 \pi} \sigma_{j}} \exp \left(-\frac{\left(x_{j}-\mu_{j}\right)^{2}}{2 \sigma_{j}^{2}}\right)$
如果$p(x)<\epsilon$，则判断x是异常点。（使用CV来选择$\epsilon$）
2. 样本选择
training set：正常点（y=0）
CV/test：多数正常点（y=0）,少数异常点（y=1）
3. Anomaly Detection  vs. Supervised Learning
- 样本类别：AD中正类样本少（y=1，异常点）负类样本多，SL中正/负类样本都很多。
- 预测效果
AD：异常点有很多类型，未来遇到的异常点可能与我们已经见过的异常都不同，算法学习起来很困难。
SL：未来遇到的positive examples和训练集已有的样本很像，预测较准确。


### learning with large datasets
lecture 17
1. 训练集90个，验证集10个，Batch_size=5（为了配合CPU/GPU内存，一般是2的N次方），epoch=3，
1个epoch时期等于用训练集中全部样本训练一次，
一个iteration等于从训练集中取batch_size个样本训练一次（这里iteration=(90*3) / 5）。

### 梯度下降优化算法
1. **Batch gradient descent**：每次iteration，使用所有m个example
![GradientDesent](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/GradientDesent.png)
![BatchGradientDescent](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/BatchGradientDescent.png)

2. **stochastic gradient descent**：每次iteration，使用1个example
![StochasticGradientDescent](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/StochasticGradientDescent.png)

3. **mini-batch gradient descent**：每次iteration，使用batch_size个example
![MiniBatchGradientDescent](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/MiniBatchGradientDescent.png)
>以上3种方法收敛情况
![3solution_compare](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/3solution_compare.png)

4. 指数加权平均-->动量梯度下降
![动量梯度下降](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/GradientDescentWithMomentum.png)
5. RMSprop
![RMSprop](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/RMSprop.png)
6. Adam
![Adam](https://cs-229-img-1300025586.cos.ap-nanjing.myqcloud.com/Adam.png)

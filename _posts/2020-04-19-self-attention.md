---
layout:     post
title:      Self-attention
subtitle:   Transformerï¼šseq2seq model with self-attention
date:       2020-04-19
author:     LT
header-img: 
catalog: true
tags:
    - seq2seq
    - 
---

>å†…å®¹æ¥è‡ª
>>[æå®æ¯…æœºå™¨å­¦ä¹ 2019-è§†é¢‘](https://www.bilibili.com/video/BV14J411W7hw?p=60)
>>[è¯¾ä»¶](http://speech.ee.ntu.edu.tw/~tlkagk/courses/ML_2019/Lecture/Transformer%20(v5).pdf)

### æ¦‚å¿µç®€ä»‹
1. transformer:  ä¸€ä¸ªæœ‰self-attentionçš„seq2seq model
2. bert:  æ— ç›‘ç£çš„è®­ç»ƒçš„transformer

### ä¸€ï¼Œattentionæƒ³è¦è§£å†³çš„æ˜¯ä»€ä¹ˆï¼Ÿ
1. æˆ‘ä»¬è¦å¤„ç†ä¸€ä¸ªseqï¼Œè‡ªç„¶æƒ³åˆ°RNNã€‚è€Œattentionæ˜¯ä¸€ç§ä»£æ›¿RNNçš„åšæ³•ã€‚
2. RNNçš„é—®é¢˜ä»¥åŠè§£å†³é—®é¢˜çš„æ¢ç´¢ï¼š
    - RNNä¸å®¹æ˜“è¢«å¹³è¡ŒåŒ–ã€‚æ¯”å¦‚ï¼Œæˆ‘è¦æƒ³è¾“å‡ºb4, æˆ‘å°±è¦å…ˆçœ‹a1,å†çœ‹a2,å†çœ‹a3,å†çœ‹a4,æ‰èƒ½è¾“å‡ºb4ã€‚æ‰€ä»¥ğŸ‘‰ç”¨CNNä»£æ›¿RNNã€‚
    - CNNä¼˜ç‚¹æ˜¯ï¼Œå¯ä»¥å¹³è¡Œã€‚å›¾ä¸­æ¯ä¸ªä¸‰è§’å½¢filteréƒ½æ˜¯å¯ä»¥åŒæ—¶æ“ä½œçš„ã€‚å½“ç„¶ï¼ŒCNNç¼ºç‚¹æ˜¯ä¸€ä¸ªfilteråªèƒ½çœ‹åˆ°è‡ªå·±è§†é‡èŒƒå›´å†…çš„ä¿¡æ¯ã€‚ä¸ºäº†è®©å®ƒçœ‹åˆ°inputé‡Œæ›´é•¿çš„ä¿¡æ¯ï¼Œå°±å¿…é¡»å¤šå å‡ å±‚ã€‚
    - ç°åœ¨ï¼Œé—®é¢˜æ˜¯ï¼Œåªæœ‰ä¸Šå±‚ä¸€äº›çš„filteræ‰èƒ½çœ‹åˆ°æ›´å¤šä¿¡æ¯ã€‚æˆ‘ä»¬æƒ³è®©ç¬¬ä¸€å±‚çš„æ¯ä¸ªfilterå°±çœ‹åˆ°æ›´å¤šä¿¡æ¯ï¼Œæ€ä¹ˆåŠï¼ŸğŸ‘‰**self-attentionï¼**
3. self-attentionå‡­ä»€ä¹ˆä»£æ›¿RNNï¼Ÿ
    - self-attentionå’ŒåŒå‘RNNæœ‰ç›¸åŒçš„èƒ½åŠ›ï¼Œå³ï¼Œæ¯ä¸ªè¾“å‡ºbiéƒ½çœ‹è¿‡äº†æ•´ä¸ªinput seqã€‚ä¸åŒä¹‹å¤„æ˜¯ï¼Œattentionæ˜¯**å¯ä»¥å¹¶è¡Œ**çš„ã€‚å³b1åˆ°b4å¯ä»¥åŒæ—¶è®¡ç®—ï¼
    - ä¸ºä»€ä¹ˆå¯ä»¥å¹¶è¡Œï¼Ÿè™½ç„¶self-attentionæ—¢ä¸æ˜¯CNNä¹Ÿä¸æ˜¯RNNï¼ˆä¸çŸ¥é“ç†è§£çš„å¯¹ä¸å¯¹ã€‚æ¬¢è¿æ‰¹è¯„æŒ‡æ­£ï¼‰ï¼Œä½†æ˜¯è¿™é‡Œé¢æœ‰ä¸€å †çŸ©é˜µä¹˜æ³•ï¼ŒçŸ©é˜µç›¸ä¹˜æ˜¯å®¹æ˜“ç”¨GPUåŠ é€Ÿçš„ã€‚

### äºŒï¼Œattentionå…·ä½“åšæ³•
   - input seqä¸­å…¶ä¸­ä¸€ä¸ªxé€šè¿‡embeddingå˜æˆaã€‚aé€šè¿‡ä¸åŒçš„matchå¾—åˆ°ä¸‰ä¸ªå‘é‡q, k, vã€‚
   - æ‹¿æ¯ä¸ªqå»å¯¹æ¯ä¸ªkåšattentionï¼ˆdot productï¼‰ã€‚å…¶ä¸­ï¼Œqå’Œkçš„ç»´åº¦ä¸€æ ·ã€‚å…¶ä¸­ï¼Œattentionåšçš„äº‹æƒ…æ˜¯ï¼Œåƒä¸¤ä¸ªå‘é‡ï¼Œoutputä¸€ä¸ªåˆ†æ•°ï¼Œè¡¨ç¤ºè¿™ä¸¤ä¸ªå‘é‡æœ‰å¤šä¹ˆåŒ¹é…ã€‚
   - ä¸¾ä¸ªä¾‹å­ã€‚æ‹¿q1å¯¹k1çš„attentionè¾“å‡ºæ˜¯Î±11ï¼Œq1å¯¹k2å¾—åˆ°Î±12ï¼Œq1å¯¹k3å¾—åˆ°Î±13ï¼Œq1å¯¹k4å¾—åˆ°Î±14ã€‚æŠŠè¿™å››ä¸ªÎ±é€è¿›softmaxå±‚ï¼Œå¾—åˆ°å››ä¸ªÎ±_hatã€‚æŠŠæ¯ä¸ªÎ±1i_hatå’Œviç›¸ä¹˜ï¼Œå¾—åˆ°å››ä¸ªå€¼ï¼Œå†ç´¯åŠ ï¼Œå¾—åˆ°ç¬¬ä¸€ä¸ªè¾“å‡ºb1ã€‚è¿™æ—¶å®ƒè€ƒè™‘äº†inputé‡Œx1åˆ°x4çš„æ‰€æœ‰ä¿¡æ¯ã€‚
   - å¦‚æœä¸æƒ³è€ƒè™‘globalçš„ä¿¡æ¯åªæƒ³è€ƒè™‘localä¿¡æ¯ï¼Œé‚£ä¹ˆæˆ‘è®©æŸä¸€ä¸ªÎ±-hatæ˜¯0å°±å¥½äº†ã€‚
   - self-attentionçš„å˜å½¢:
       * multi-headÂ  self-attentionï¼šä¸åŒçš„headå…³æ³¨ç‚¹ä¸ä¸€æ ·ï¼Œæœ‰ç‚¹æƒ³çœ‹localä¿¡æ¯ï¼Œæœ‰ç‚¹æƒ³çœ‹globalçš„ã€‚

### ä¸‰ï¼Œå­˜åœ¨çš„é—®é¢˜å’Œæ–¹æ³•ï¼špositional encoding
   - self-attentionæ²¡æœ‰è€ƒè™‘ä½ç½®ä¿¡æ¯ã€‚â€”â€”> æ‰‹å·¥è®¾è®¡å’ŒaiåŒç»´åº¦çš„eiï¼ˆpositional encodingï¼‰ï¼Œå’ŒaiåŠ èµ·æ¥ã€‚

### å››ï¼Œself-attentionåœ¨seq2seqé‡Œçš„åº”ç”¨
- seq2seqé‡Œï¼Œencoderå’Œdecoderåˆ†åˆ«ç”¨self-attentionå–ä»£ ğŸ‘‰transformerã€‚
- æˆ‘ä»¬ç†ŸçŸ¥çš„batch normalizationæ˜¯ï¼ŒåŒä¸€ä¸ªç»´åº¦ä¸Šçš„ä¸åŒdataåšå½’ä¸€åŒ–ï¼ˆä½¿å¾—å‡å€¼ä¸º0æ–¹å·®ä¸º1ï¼‰
- è¿™é‡Œé¢ç”¨åˆ°layer normalizationï¼Œæ„æ€æ˜¯ï¼Œå¯¹åŒä¸€ç¬”dataçš„ä¸åŒç»´åº¦çš„å€¼å½’ä¸€åŒ–ã€‚layerå½’ä¸€åŒ–ä¸€èˆ¬æ­é…RNNä½¿ç”¨ã€‚è€Œtransformerå¾ˆåƒRNNï¼Œè¿™å¯èƒ½æ˜¯transformerç”¨layer normalizationçš„ç†ç”±ã€‚
- åº”ç”¨ä¸¾ä¾‹ï¼š
    * è¾“å…¥å¾ˆå¤šæ–‡ç« ï¼Œè¾“å‡ºæ‘˜è¦ã€‚ã€‚
    * å¯¹äºå¥å­`the dog didn't cross the street because it was too tired`ï¼Œattentionèƒ½å­¦å‡º`it`æŒ‡ä»£çš„å¯¹è±¡æ˜¯`dog`ã€‚è€Œå¯¹äºå¥å­`the dog didn't cross the street because it was too wide`ï¼Œattentionèƒ½å­¦å‡º`it`æŒ‡ä»£çš„å¯¹è±¡æ˜¯`street`ã€‚
